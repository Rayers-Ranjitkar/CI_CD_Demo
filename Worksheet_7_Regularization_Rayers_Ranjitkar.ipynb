{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Pz-hLcbRvkV3C9rzXpcZH6WcJitE0Brs",
      "authorship_tag": "ABX9TyNEv7T3y86b24jXSRZPAC0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rayers-Ranjitkar/CI_CD_Demo/blob/main/Worksheet_7_Regularization_Rayers_Ranjitkar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/housing.csv')\n",
        "df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "7rVQysmc89yD",
        "outputId": "2050aed9-1e7c-4964-e586-bd18cfe7d16c"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -122.23     37.88                41.0        880.0           129.0   \n",
              "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
              "\n",
              "   population  households  median_income  median_house_value ocean_proximity  \n",
              "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
              "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a025f314-4713-40a9-a926-fd4e78348505\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "      <th>ocean_proximity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.23</td>\n",
              "      <td>37.88</td>\n",
              "      <td>41.0</td>\n",
              "      <td>880.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>8.3252</td>\n",
              "      <td>452600.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-122.22</td>\n",
              "      <td>37.86</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7099.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>1138.0</td>\n",
              "      <td>8.3014</td>\n",
              "      <td>358500.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a025f314-4713-40a9-a926-fd4e78348505')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a025f314-4713-40a9-a926-fd4e78348505 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a025f314-4713-40a9-a926-fd4e78348505');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8aba1855-add9-4733-87c8-4a4bff692aff\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8aba1855-add9-4733-87c8-4a4bff692aff')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8aba1855-add9-4733-87c8-4a4bff692aff button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20640,\n  \"fields\": [\n    {\n      \"column\": \"longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.003531723502581,\n        \"min\": -124.35,\n        \"max\": -114.31,\n        \"num_unique_values\": 844,\n        \"samples\": [\n          -118.63,\n          -119.86,\n          -121.26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1359523974571117,\n        \"min\": 32.54,\n        \"max\": 41.95,\n        \"num_unique_values\": 862,\n        \"samples\": [\n          33.7,\n          34.41,\n          38.24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing_median_age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.585557612111637,\n        \"min\": 1.0,\n        \"max\": 52.0,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          35.0,\n          25.0,\n          7.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_rooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2181.615251582787,\n        \"min\": 2.0,\n        \"max\": 39320.0,\n        \"num_unique_values\": 5926,\n        \"samples\": [\n          699.0,\n          1544.0,\n          3966.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_bedrooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 421.3850700740322,\n        \"min\": 1.0,\n        \"max\": 6445.0,\n        \"num_unique_values\": 1923,\n        \"samples\": [\n          1538.0,\n          1298.0,\n          1578.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"population\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1132.4621217653375,\n        \"min\": 3.0,\n        \"max\": 35682.0,\n        \"num_unique_values\": 3888,\n        \"samples\": [\n          4169.0,\n          636.0,\n          3367.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"households\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 382.3297528316099,\n        \"min\": 1.0,\n        \"max\": 6082.0,\n        \"num_unique_values\": 1815,\n        \"samples\": [\n          21.0,\n          750.0,\n          1447.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8998217179452732,\n        \"min\": 0.4999,\n        \"max\": 15.0001,\n        \"num_unique_values\": 12928,\n        \"samples\": [\n          5.0286,\n          2.0433,\n          6.1228\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median_house_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115395.6158744132,\n        \"min\": 14999.0,\n        \"max\": 500001.0,\n        \"num_unique_values\": 3842,\n        \"samples\": [\n          194300.0,\n          379000.0,\n          230100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocean_proximity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"<1H OCEAN\",\n          \"ISLAND\",\n          \"INLAND\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1**"
      ],
      "metadata": {
        "id": "FXJwERww3Fw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the model\n",
        "\n",
        "X = df.drop('median_house_value', axis=1)\n",
        "y = df['median_house_value']\n",
        "\n",
        "# X.head()\n",
        "X = pd.get_dummies(X, drop_first=True) #one-hot-encoding to make ocean_promixity column values a number\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "emvblP-F9DbC"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline Model:"
      ],
      "metadata": {
        "id": "qftA1O7q3OZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline Linear Regression (no regularization)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "baseline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # imputer fills NaNs with column mean as LinearRegression cannot handle NaNs\n",
        "    (\"model\", LinearRegression())  # Trains a linear regression model on the data AFTER NaNs are filled\n",
        "])\n",
        "\n",
        "baseline.fit(X_train, y_train) # Fits the model using the baseline that has the pipeline\n",
        "\n",
        "train_pred = baseline.predict(X_train)\n",
        "test_pred  = baseline.predict(X_test)\n",
        "\n",
        "train_mse = mean_squared_error(y_train, train_pred)\n",
        "test_mse  = mean_squared_error(y_test, test_pred)\n",
        "\n",
        "print(\"Baseline Linear Regression (with imputer)\")\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test  MSE:\", test_mse)\n",
        "\n",
        "# coefficients come from the model inside the pipeline\n",
        "print(\"Coefficients:\", baseline.named_steps[\"model\"].coef_)\n",
        "\n",
        "\n",
        "# Interpretation of MSE:\n",
        "# The training MSE is slightly lower than the test MSE.\n",
        "# This indicates that the model performs better on the data it was trained on\n",
        "# compared to unseen data suggesting that the model may be overfitting.\n",
        "# Moreover, The relatively large coefficients suggests that the model is sensitive to certain features and higher test MSE indicate that the model may overfit the training data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3weILvmATEB",
        "outputId": "8082d275-0d80-48da-8a2c-a51a94a7649f"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Linear Regression (with imputer)\n",
            "Train MSE: 4683203783.504253\n",
            "Test  MSE: 4904409297.414918\n",
            "Coefficients: [-2.68382734e+04 -2.54683520e+04  1.10218508e+03 -6.02150567e+00\n",
            "  1.02789395e+02 -3.81729064e+01  4.82527528e+01  3.94739752e+04\n",
            " -3.97866562e+04  1.36125073e+05 -5.13664222e+03  3.43114007e+03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-Paramter Tuning and Regularization:\n"
      ],
      "metadata": {
        "id": "lJGlkEzT3SJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Hyperparameter Tuning\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Defining a grid of alpha values\n",
        "# alpha controls how strong regularization is:\n",
        "alphas = np.logspace(-4, 2, 20)  # 10^-4 to 10^2(20 values)\n",
        "\n",
        "# RIDGE (L2 regularization)\n",
        "\n",
        "ridge_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),   # fills NaNs\n",
        "    (\"scaler\", StandardScaler()),                  # scales features for fair regularization\n",
        "    (\"model\", Ridge())\n",
        "])\n",
        "\n",
        "ridge_grid = GridSearchCV(\n",
        "    ridge_pipe,\n",
        "    param_grid={\"model__alpha\": alphas}, # - tries all alpha values and selects alpha that gives the best average CV performance\n",
        "    cv=5, #5-folds cross validation\n",
        "    scoring=\"neg_mean_squared_error\"  # the best model is the one with the highest (least negative) neg-MSE, which corresponds to the lowest MSE.\n",
        ")\n",
        "\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Ridge alpha:\", ridge_grid.best_params_[\"model__alpha\"])\n",
        "print(\"Best Ridge CV neg-MSE:\", ridge_grid.best_score_)\n",
        "\n",
        "# Evaluate best Ridge model on test set\n",
        "best_ridge = ridge_grid.best_estimator_\n",
        "\n",
        "ridge_train_mse = mean_squared_error(y_train, best_ridge.predict(X_train))\n",
        "ridge_test_mse  = mean_squared_error(y_test,  best_ridge.predict(X_test))\n",
        "\n",
        "print(\"\\nRidge Results\")\n",
        "print(\"Train MSE:\", ridge_train_mse)\n",
        "print(\"Test  MSE:\", ridge_test_mse)\n",
        "\n",
        "ridge_coef = best_ridge.named_steps[\"model\"].coef_\n",
        "print(\"First 10 Ridge coefficients:\", ridge_coef[:10])\n",
        "# L2 has shrinked coefficients but rarely makes them exactly 0. It does not perform feature selectio and All features remain in the model.\n",
        "# Positive coefficient → feature increases prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i85e3yjDbiF",
        "outputId": "b1902613-63bc-4f10-fd25-dbb67583d909"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Ridge alpha: 11.288378916846883\n",
            "Best Ridge CV neg-MSE: -4710741160.328094\n",
            "\n",
            "Ridge Results\n",
            "Train MSE: 4683326738.187337\n",
            "Test  MSE: 4900076762.299355\n",
            "First 10 Ridge coefficients: [-52583.18569496 -53124.74647825  13896.27181038 -12700.4299197\n",
            "  42091.02958094 -43247.89499348  18837.18843812  75054.78928764\n",
            " -18865.50886112   2127.27334362]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LASSO (L1 regularization)\n",
        "\n",
        "\n",
        "lasso_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", Lasso(max_iter=50000))\n",
        "])\n",
        "\n",
        "lasso_grid = GridSearchCV(\n",
        "    lasso_pipe,\n",
        "    param_grid={\"model__alpha\": alphas},\n",
        "    cv=5,\n",
        "    scoring=\"neg_mean_squared_error\"\n",
        ")\n",
        "\n",
        "lasso_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Lasso alpha:\", lasso_grid.best_params_[\"model__alpha\"])\n",
        "print(\"Best Lasso CV neg-MSE:\", lasso_grid.best_score_)\n",
        "\n",
        "# Evaluate best Lasso model on test set\n",
        "best_lasso = lasso_grid.best_estimator_\n",
        "\n",
        "lasso_train_mse = mean_squared_error(y_train, best_lasso.predict(X_train))\n",
        "lasso_test_mse  = mean_squared_error(y_test,  best_lasso.predict(X_test))\n",
        "\n",
        "print(\"\\nLasso Results\")\n",
        "print(\"Train MSE:\", lasso_train_mse)\n",
        "print(\"Test  MSE:\", lasso_test_mse)\n",
        "\n",
        "# Observe coefficients (L1 can set some coefficients exactly to 0 -> feature selection)\n",
        "lasso_coef = best_lasso.named_steps[\"model\"].coef_\n",
        "print(\"First 10 Lasso coefficients:\", lasso_coef[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71jFDAWRHE8r",
        "outputId": "090fe9c6-d815-48fa-f547-16a68fc41801"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Lasso alpha: 23.357214690901213\n",
            "Best Lasso CV neg-MSE: -4711053969.781619\n",
            "\n",
            "Lasso Results\n",
            "Train MSE: 4683266248.712713\n",
            "Test  MSE: 4902411232.053286\n",
            "First 10 Lasso coefficients: [-53151.16767293 -53737.25044649  13874.29111623 -12604.10922936\n",
            "  42537.7135272  -43232.02067988  18272.97738518  75052.44351367\n",
            " -18706.9794461    2099.95250623  -1545.01658692   1164.6138847 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test MSE Comparison (lower is better)\")\n",
        "print(\"Ridge Test MSE:\", ridge_test_mse)\n",
        "print(\"Lasso Test MSE:\", lasso_test_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ybmBPr7JKTD",
        "outputId": "5bb9461e-dfb2-489a-cfae-76f0ec5209a7"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE Comparison (lower is better)\n",
            "Ridge Test MSE: 4900076762.299355\n",
            "Lasso Test MSE: 4902411232.053286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE comparison:\n",
        "# Both Ridge and Lasso models show similar training and test MSE values.\n",
        "# Compared to the baseline model, the gap between training and test MSE\n",
        "# is reduced, indicating improved generalization.\n",
        "# This suggests that regularization helps control overfitting.\n"
      ],
      "metadata": {
        "id": "eq3ZXQ0ZJo0U"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (a) How L1 / L2 reduces variance and prevents overfitting\n",
        "\n",
        "# By shrinking coefficient values, the model becomes less sensitive\n",
        "# to noise in the training data, which helps prevent overfitting.\n",
        "# Ridge reduces variance by shrinking all coefficients, while Lasso\n",
        "# reduces variance by shrinking and sometimes removing features entirely.\n"
      ],
      "metadata": {
        "id": "8BzWOacfJplm"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (b) How excessive regularization may increase bias and underfit\n",
        "\n",
        "# If the regularization strength (alpha) is too large, the model\n",
        "# becomes overly simple. Also, Important features may be overly penalized or removed, leading to higher bias and underfitting.\n",
        "# This results in higher errors on both training and test datasets."
      ],
      "metadata": {
        "id": "CVcbOIUxJu1e"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2**"
      ],
      "metadata": {
        "id": "wjhIOrF7K5NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "6CKzccUYKmwq"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline Model"
      ],
      "metadata": {
        "id": "BNgelIC33dRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline Logistic Regression (no explicit regularization tuning)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "baseline_logreg = Pipeline([\n",
        "    (\"scaler\", StandardScaler()), # 1) StandardScaler -> scales features (important for Logistic Regression)\n",
        "    (\"model\", LogisticRegression(max_iter=10000))\n",
        "])\n",
        "\n",
        "# Training the model on the training set\n",
        "baseline_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Observing coefficients of the logistic regression model\n",
        "coefficients = baseline_logreg.named_steps[\"model\"].coef_\n",
        "\n",
        "print(\"Coefficient shape:\", coefficients.shape)\n",
        "print(\"First 10 coefficients:\", coefficients[0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CpxWXxOLJhg",
        "outputId": "6b73e543-6d66-4199-8e4b-ca8e1f2c5a60"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient shape: (1, 30)\n",
            "First 10 coefficients: [-0.43190368 -0.38732553 -0.39343248 -0.46521006 -0.07166728  0.54016395\n",
            " -0.8014581  -1.11980408  0.23611852  0.07592093 -1.26817815  0.18887738\n",
            " -0.61058302 -0.9071857  -0.31330675  0.68249145  0.17527452 -0.3112999\n",
            "  0.50042502  0.61622993 -0.87984024 -1.35060559 -0.58945273 -0.84184594\n",
            " -0.54416967  0.01611019 -0.94305313 -0.77821726 -1.20820031 -0.15741387]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observation of coefficients:\n",
        "# The coefficients indicate the influence of each feature on the predicted class.\n",
        "# A positive coefficient increases the probability of the positive class,\n",
        "# while a negative coefficient decreases it.\n",
        "# Large coefficient values indicate features with stronger influence.\n"
      ],
      "metadata": {
        "id": "XmnaXCtuLXhr"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "train_pred = baseline_logreg.predict(X_train)\n",
        "test_pred  = baseline_logreg.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "train_acc = accuracy_score(y_train, train_pred)\n",
        "test_acc  = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7V8BuzPL4rC",
        "outputId": "49ef6b58-e9d7-4f5d-ada9-5222eb5a2e34"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9868131868131869\n",
            "Test Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training accuracy is much higher than test accuracy, indicating overfitting."
      ],
      "metadata": {
        "id": "07N-R-P1MHQi"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparamter Tuning"
      ],
      "metadata": {
        "id": "Mm6EK7jH4nVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning using GridSearchCV (Logistic Regression)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Defining grid of hyperparameters\n",
        "C_values = np.logspace(-4, 4, 20)  # 10^-4 to 10^2(20 values) # - small C  -> strong regularization # - large C  -> weak regularization\n",
        "\n",
        "param_grid = {\n",
        "    \"model__C\": C_values, # Regularization strength values to try during grid search\n",
        "    \"model__penalty\": [\"l1\", \"l2\"] # Type of regularizations\n",
        "}\n",
        "\n",
        "logreg_pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()), # Standardizes features of each column by calculating each mean and sd deviation [(x-mean)/std ]\n",
        "    (\"model\", LogisticRegression(\n",
        "        solver=\"liblinear\",  # algorithmn that supports both l1 and l2\n",
        "        max_iter=10000\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 2) GridSearchCV with cross-validation on training set\n",
        "logreg_grid = GridSearchCV(\n",
        "    logreg_pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                 # 5-fold cross-validation\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "logreg_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best hyperparameters:\", logreg_grid.best_params_) #?\n",
        "print(\"Best CV accuracy:\", logreg_grid.best_score_)\n",
        "\n",
        "# 3) Evaluating best model on test set\n",
        "best_logreg = logreg_grid.best_estimator_\n",
        "\n",
        "test_pred = best_logreg.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-hdQCtCMBPU",
        "outputId": "5ea56dc0-2576-4ef8-ebf6-6206a87644ab"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'model__C': np.float64(0.08858667904100823), 'model__penalty': 'l2'}\n",
            "Best CV accuracy: 0.9780219780219781\n",
            "Test Accuracy: 0.9912280701754386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After hyperparameter tuning using GridSearchCV, the test accuracy improved\n",
        "# from the baseline model i.e. from 97% to 99%. The optimized logistic regression model achieved\n",
        "# higher accuracy on the test set, indicating better generalization to unseen data."
      ],
      "metadata": {
        "id": "2RAsGkccNLN3"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization"
      ],
      "metadata": {
        "id": "Yq-t7pbL7r6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization Experiments (L1 vs L2)\n",
        "\n",
        "#Train L2 (Ridge-like)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Using optimal C from GridSearchCV\n",
        "best_C = 0.08858667904100823\n",
        "\n",
        "# L2 Regularization (Ridge-like)\n",
        "l2_model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(\n",
        "        C=best_C,\n",
        "        penalty=\"l2\",\n",
        "        solver=\"liblinear\",\n",
        "        max_iter=10000\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train L2 model\n",
        "l2_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "l2_train_pred = l2_model.predict(X_train)\n",
        "l2_test_pred  = l2_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "l2_train_acc = accuracy_score(y_train, l2_train_pred)\n",
        "l2_test_acc  = accuracy_score(y_test, l2_test_pred)\n",
        "\n",
        "print(\"L2 Train Accuracy:\", l2_train_acc)\n",
        "print(\"L2 Test Accuracy:\", l2_test_acc)\n",
        "\n",
        "# Coefficients\n",
        "l2_coef = l2_model.named_steps[\"model\"].coef_[0]\n",
        "print(\"First 10 L2 coefficients:\", l2_coef[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewl4OP9MNhTP",
        "outputId": "696532d3-d781-4bc1-bd12-1bd4dfc4fcc2"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Train Accuracy: 0.9824175824175824\n",
            "L2 Test Accuracy: 0.9912280701754386\n",
            "First 10 L2 coefficients: [-0.35044312 -0.38779711 -0.34126498 -0.36576375 -0.12469509  0.01131365\n",
            " -0.36227677 -0.46083554 -0.03397208  0.15606202]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observation (L2 Regularization):\n",
        "# L2 regularization shrinks all coefficients toward zero but rarely makes them\n",
        "# exactly zero. All features remain in the model, but their influence is reduced.\n",
        "# This helps control model complexity and reduce overfitting."
      ],
      "metadata": {
        "id": "C4rk_tZiN6i7"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L1 Regularization (Lasso-like)\n",
        "l1_model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(\n",
        "        C=best_C,\n",
        "        penalty=\"l1\",\n",
        "        solver=\"liblinear\",\n",
        "        max_iter=10000\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train L1 model\n",
        "l1_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "l1_train_pred = l1_model.predict(X_train)\n",
        "l1_test_pred  = l1_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "l1_train_acc = accuracy_score(y_train, l1_train_pred)\n",
        "l1_test_acc  = accuracy_score(y_test, l1_test_pred)\n",
        "\n",
        "print(\"\\nL1 Train Accuracy:\", l1_train_acc)\n",
        "print(\"L1 Test Accuracy:\", l1_test_acc)\n",
        "\n",
        "# Coefficients\n",
        "l1_coef = l1_model.named_steps[\"model\"].coef_[0]\n",
        "print(\"First 10 L1 coefficients:\", l1_coef[:10])\n",
        "\n",
        "# Count zero coefficients (sparsity)\n",
        "print(\"Number of zero coefficients (L1):\",\n",
        "      np.sum(l1_coef == 0), \"out of\", len(l1_coef))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQopi3ZgN8d-",
        "outputId": "5b243917-a3b1-4864-a424-cde92adfdad0"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "L1 Train Accuracy: 0.9802197802197802\n",
            "L1 Test Accuracy: 0.9649122807017544\n",
            "First 10 L1 coefficients: [ 0.          0.          0.          0.          0.          0.\n",
            "  0.         -0.99384906  0.          0.        ]\n",
            "Number of zero coefficients (L1): 22 out of 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observation (L1 Regularization):\n",
        "# L1 regularization encourages sparsity by pushing some coefficients exactly to zero.\n",
        "# This effectively performs feature selection by removing less important features.\n",
        "# Compared to L2, L1 produces a simpler and more interpretable model."
      ],
      "metadata": {
        "id": "3OE5JBDZOA9_"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy Comparison\")\n",
        "print(\"L2 -> Train:\", l2_train_acc, \" Test:\", l2_test_acc)\n",
        "print(\"L1 -> Train:\", l1_train_acc, \" Test:\", l1_test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyLNjUkXOFOE",
        "outputId": "83c949b9-bb13-4e89-d1d6-b3c8e2a8419a"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison\n",
            "L2 -> Train: 0.9824175824175824  Test: 0.9912280701754386\n",
            "L1 -> Train: 0.9802197802197802  Test: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy comparison:\n",
        "# L2 regularization achieves higher test accuracy compared to L1.\n",
        "# This indicates better generalization when all features are retained\n",
        "# with reduced influence.\n",
        "# L1 regularization leads to lower test accuracy since, L1 tries to push coefficients to exactly zero which suggesting that\n",
        "# removing some features increases bias and causes slight underfitting.\n"
      ],
      "metadata": {
        "id": "XT4P9rCQOGmT"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observation:\n",
        "# L1 and L2 regularization both reduce variance by limiting model complexity.\n",
        "# However, L1 enforces sparsity by removing features, which can increase bias if important features are discarded.\n",
        "# In this experiment, L2 provides a better bias–variance balance, while L1 introduces higher bias and reduces test accuracy\n",
        "# which indicates that most features are informative and should not be removed."
      ],
      "metadata": {
        "id": "lex36_MyOog_"
      },
      "execution_count": 116,
      "outputs": []
    }
  ]
}